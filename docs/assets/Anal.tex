\documentclass[11pt]{scrartcl}
\usepackage[sexy]{evan}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{array}
% \usepackage{program}
% \usepackage{algorithm}
% \usepackage{amsmath}
% \usepackage{algpseudocode}
\begin{document}

\title{Some Analysis Things}
\author{Arthur Conmy\footnote{Please send any corrections and/or feedback to \url{asc70@cam.ac.uk}.}}
\date{Part IB, Easter Term 2021}

\maketitle
\begin{abstract}
In these notes, I make some brief comments on the IB Analysis courses\footnote{strictly, just Analysis and Topology and Complex Analysis, but I hoep that the reader agrees that Analysis and Topology is ... more than one courses worth of material!}.

Credit is due to Evan Chen for the style file for these notes\footnote{Available here: \url{https://github.com/vEnhance/dotfiles/blob/master/texmf/tex/latex/evan/evan.sty}.}.
\end{abstract}

\section{Integration}

% $h(x) = \inf_{m \ge n} f_m(x)$.
% $g(x) = \liminf_{n \to \infty} f_n(x)$.

\begin{theorem}
[Interchanging Differentiation and Integration]
    Let $f:\RR \times [0, 1] \rightarrow \RR$  be a cts function of $\theta$ and $t$, and suppose $\frac{\partial f}{\partial \theta}$ is also cts.

    Then 

    \begin{equation}
        \frac{d}{d\theta} \int_0^1 f(\theta, t) dt = \int_0^1 \frac{\partial f}{\partial \theta}(\theta, t) dt.
    \end{equation}

\begin{proof}
    The key idea is that differentiability is a local property, so we can force the domain of $\frac{\partial f}{\partial \theta}$ to be compact.

    WLOG let $\theta = 0$. Now $\forall \varepsilon > 0$, pick $\delta > 0$ so that $\left| \frac{\partial f}{\partial \theta}(\theta, t) -  \frac{\partial f}{\partial \theta} (0, t) \right| < \varepsilon$ for $(\theta, t) \in [-\delta,\delta] \times [0,1]$. Then let

    \begin{equation}
        F(\theta) = \int_0^1 f(\theta, t) dt.
    \end{equation}

    Then we can directly calculate (for $|h| < \delta$)

    \begin{equation}
        \frac1h (F(h) - F(0))) = \int_0^1 \frac{f(h, t) - f(0, t)}{h} dt = \int_0^1 \frac{\partial f}{\partial \theta}(\theta_t, t) dt.
    \end{equation}

    where the last equality follows from the mean value theorem. $\theta_t \in (-|h|, |h|)$ is a function of $t$\footnote{Extra: can we always make it a \emph{cts} function of $t$?}. %, by the continuity of $\frac{\partial f}{\partial \theta}$.

    But our choice of delta means that this differs by at most $\varepsilon$ from

    \begin{equation}
        \int_0^1 \frac{\partial f}{\partial \theta}(0, t) dt,
    \end{equation}

    so we're done.

    % \begin{equation}
        % \left| \frac{\partial f}{\partial \theta}(t, \theta_t) - \frac{\partial f}{\partial \theta}(t, \theta) \right|
    % \end{equation}

    % $[\theta_0 - \delta, \theta_0 + \delta] \times [a, b]$ is compact, and hence $\frac{\partial f}{\partial \theta}$ is bounded on it, say by $M$. So $\forall \varepsilon > 0$ we can make $\delta$ sufficiently small so tha
\end{proof}
\label{switcheroo}
\end{theorem}

\begin{theorem}
[CIF for Derivatives]

Let $U$ be a domain and $f : U \to \CC$ holomorphic. Let $D(0, 1) \subseteq U$ and $w \in D(0, 1)$. Then

\begin{equation}
    f^{(n)}(w) = \frac{1}{n!} \oint_{\partial D(0, 1)} \frac{f(z)}{(z-w)^{n+1}} dz.
\end{equation}

\begin{proof}
    Case $n=1$ is the ordinary integral formula, and we show how the $n=2$ arises by applying (\ref{switcheroo}). The higher order cases arise similarly.

    We can write the $n=1$ case as

    \begin{equation}
        \oint_{\partial D(0, 1)} \frac{f(z)}{z-w} dz = \int_0^1 \frac{f(\gamma(t))\gamma'(t)}{\gamma(t)-w} dt.
    \end{equation}

    where $\gamma(t) = e^{2 \pi i t}$. We can now directly apply the previous result, since the integrand's partial $w$ derivative is indeed cts, provided we localise to a ball around $w$ (so that the $\frac{1}{\gamma(t)-w}$ term doesn't get very large).
\end{proof}
\end{theorem}

% \begin{proof}
    % Deduce the 
% \end{proof}

\section{Differentiation}

I don't think this part of the course needs to be anywhere near as feared as it is currently.

\begin{definition}
[Norm]

Let $V$ be a real vector space. A norm on $V$ is a function $||.|| : V \rightarrow \RR$ satisfying

\begin{itemize}
\item $||v|| \ge 0$, with equality iff $v=0$.
\item $||v+w|| \le ||v|| + ||w||$.
\item $||\lambda v|| = |\lambda| ||v||$.
\end{itemize}
\end{definition}

\begin{remark}
This naturally induces a topology on $V$ by turning it into a metric space with distance function $d(v,w) = ||v-w||$.
\end{remark}

\begin{theorem}
[Only one norm]

Let $V$ be a finite dimensional vector space. Then all norms on $V$ are Lipschitz equivalent.

\begin{proof}

Fix a basis $e_1, ... , e_n$ of $V$. Then let $||.||_2$ be the Euclidean norm, i.e

\begin{equation}
    ||\lambda_1 e_1 + .. + \lambda_n e_n||_2 = \sqrt{ \lambda_1^2 + ... + \lambda_n^2 }.
\end{equation}

We show that all norms are Lipschitz equivalent to the Euclidean norm, and since Lipschitz equivalence is an equivalence relation, this will suffice.

\begin{itemize}
    \item $\exists m > 0$ such that $||v|| \le m ||v||_2$ for all $v \in V$:
    
    Direct application of the triangle inequality. Let 

    \begin{equation}
        E = \max_{i=1}^n ||e_i|| > 0.
    \end{equation}

    Then if $v = \lambda_1 e_1 + ... + \lambda_n e_n$, and

    \begin{equation}
        \Lambda = \max_{i=1}^n |\lambda_i|,
    \end{equation}

    then 

    \begin{equation}
        ||v|| \le |\lambda_1| ||e_1|| + ... + |\lambda_n| ||e_n|| \le E (|\lambda_1| + |\lambda_2| + ... + |\lambda_n|)
    \end{equation}

    where the first inequality follows from the triangle inequality. So

    \begin{equation}
        ||v|| \le nE\Lambda.
    \end{equation}

    Now $||v||_2 \ge \Lambda$ and hence $m = nE$ works (note we need $\Lambda$ independence, but $E$ dependence is fine since the former is a property of the specific $v$, but the latter a property of the norm).

    \item $\exists M > 0$ such that $||v|| \ge M ||v||_2$ for all $v \in V$:
    
    For this direction, consider the identity map 

    \begin{equation}
        \iota : (V, ||.||_2) \rightarrow (V, ||.||)
    \end{equation}

    between metric spaces. This is continuous, by the other direction of this proof. 
    
    Consider the image of the set of points $S = \{  v \in V : ||v||_2 = 1 \}$. This is a compact set, since we can check that sequences have convergent subsequences since all $e_i$ components will have values that vary in some bounded range.

    Now the cts image of a compact set is compact, There is a minimum value of $||\iota(v)||$ \textit{that is achieevd} for some $v \in V$, and cannot be 0 because of the definition of norm.

    Then this is exactly the $M$ value needed for the proof.
\end{itemize}
\end{proof}
\end{theorem}

Why does this matter? Recall the definition of differentiablility

\begin{definition}

$f: \RR^m \to \RR^n$ is differentiable at $p$ with derivative the linear map $T(p)(h)$ if 

\begin{equation}
    \lim_{h : ||h||_2 \rightarrow 0} \frac{ || f(p + h) - f(p) - T(p)(h) ||_2 }{||h||_2}.
\end{equation}

\label{derivvers}
% where the limit is taken over $h \in \RR^m$
\end{definition}

The important thing to notice here is that while $f$ and $T(p)$ are both maps from $\RR^m$, the former exclusively maps from points very close to $p$, and the latter exclusively maps from points very close to 0. If this isn't clear, chapter 42 of \cite{Napkin} (Napkin) provides a far better explanation than what I can give (and includes pictures!).

To be concrete however, working with a multi-dimensional limit is hard. In practice, we are likely to generally use the following result (cross-posted from my Geometry notes).

\begin{theorem}
    [Computing derivatives in $n$ dimensions.]
        Suppose that $U \subseteq^\circ \RR^m$ and $f : U \rightarrow \RR$ has continuous partial derivatives at $p \in U$. Then $f$ is differentiable at $p$, with derivative
        
        \begin{equation}
            Df(p)(h) = \frac{\partial f}{\partial x_1}h_1 + \cdots + \frac{\partial f}{\partial x_m}h_m.
            \label{Deriv1}
        \end{equation}
    
        \begin{proof}
            Decompose as a telescoping sum
    
            \begin{align}
            f(x_1 + h_1, \cdots , x_m + h_m) - f(x_1, \cdots , x_m) \\ 
            = f(x_1 + h_1, \cdots , x_m + h_m) - f(x_1 + h_1, \cdots , x_{m-1} + h_{m-1} , x_m) \\
            + f(x_1 + h_1, \cdots , x_{m-1} + h_{m-1}, x_m) - f(x_1 + h_1, \cdots , x_{m-1}, x_m) \\
            + \cdots \\
            + f(x_1 + h_1, x_2, \cdots , x_m) - f(x_1, \cdots , x_m).
            \end{align}
    
            And now by an `$m$-$\varepsilon$' proof (i.e $m$ applications of the triangle inequality), we use continuity of the $m$ partial derivatives to deduce the desired derivative expression.

            However, actually doing this is somewhat more subtle than it may seem at first (for example, to me!), since the limit $||h||_2 \to 0$ need be independent of the relative `speeds' that each $h$ component approach 0.

            We illustrate how we do this in the case $m=2$; the argument is easily generalised to the higher order cases.

            \begin{example}
                WLOG show differentiability at 0.

                The quantity $Q$ that we need show approaches 0 as $h_1 \to 0$ and $h_2 \to 0$ is

                \begin{equation}
                    Q = \frac{f(h_1, h_2) - f(0, 0) - h_1 \frac{\partial f}{\partial x}|_0 - h_2 \frac{\partial f}{\partial y}|_0}{\sqrt{h_1^2 + h_2^2}}
                    \label{Q equation}
                \end{equation}

                Now by the triangle inequality, and that $\sqrt{h_1^2 + h_2^2} \ge |h_1|, |h_2|$,

                \begin{equation}
                    |Q| \le \left| \frac{f(h_1,h_2) - f(0, h_2) - h_1 \frac{\partial f}{\partial x}|_0}{h_1} \right| + \left| \frac{f(0,h_2) - f(0, 0) - h_2 \frac{\partial f}{\partial y}|_0}{h_2} \right|
                \end{equation}

                as $h_2 \to 0$, clearly the latter term approaches 0 (this is easy due to the remark (\ref{minus one remark}) made below). However, for the first term the `different speeds' issue immediately arises. To remedy this, what we can do is localise to an open set $U \oset \RR^2$ around 0 so that when $(x,y) \in U$,

                \begin{equation}
                    \left| \frac{\partial f}{\partial x}|_{(x,y)} - \frac{\partial f}{\partial x}|_0 \right| < \eps.
                \end{equation}

                Then this means that if $(h_1, h_2) \in U$,

                \begin{equation}
                    \frac{f(h_1, h_2) - f(0, h_2)}{h_1}
                \end{equation}

                differs from $\frac{\partial f}{\partial x}|_0$ by at most $\eps$, since otherwise by the \emph{mean value theorem}, somwhere in $U$ we would have $\frac{\partial f}{\partial x}$ differing by more than $\eps$ from $\frac{\partial f}{\partial x}|_0$.

            \end{example}
        \end{proof}
    \label{extracting derivatives}
\end{theorem}

\begin{remark} 
This is basically the same sort of thing we did in IA DEs when we integrated from $(x_1, y_1)$ to $(x_2, y_2)$ by first travelling from $(x_1, y_1)$ to $(x_2, y_1)$ (with fixed $y$ value) and then from $(x_2, y_1)$ to $(x_2, y_2)$ (with fixed $x$ value), like a staircase.
\end{remark}

\begin{remark}
    Actually, the last term in the telescoping series, $f(x_1 + h_1, x_2, \cdots , x_m) - f(x_1, \cdots , x_m)$ \emph{is} the partial derivative (in $x_1$) of $f$ at $(x_1, ... , x_n)$. So we only need continuity of $n-1$ of the partial derivatives at a point, and existence of the last, in order to deduce differentiablility at a point.
    \label{minus one remark}
\end{remark}

An important application of this is the following \textit{sufficient} condition for a function to be complex differentiable:

\begin{theorem}
[Holomorphic sufficiency]

Suppose that the real and imaginary parts of $f = u + vi$ satisfy the Cauchy-Riemann equations, i.e $u_x = v_y$ and $u_y =  - v_x$. Then if $u$ and $v$ are $C^1$, then $f$ is holomorphic.

\begin{proof}

We want to follow the proof methodology of (\ref{extracting derivatives}) as close as possible. In particular, let's just show differentiability at 0.

This means that since $f : \CC \to \CC$ is essentially a map $\RR^2 \to \RR^2$, we want to consider its components, so we only need deal with functions $\RR^2 \to \RR$. This is of course is exactly the same thing as considering real and imaginary parts.

We `know' that $f' = u_x + i v_x$ from computing the limit only in the real direction. In addition, because all of our limits will be over a denominator of $h_1 + ih_2$ we therefore write

\begin{equation}
    u_x + i v_x = \frac{ (u_x + i v_x)(h_1 + i h_2) }{h_1 + i h_2} = \frac{(u_x h_1 - v_x h_2) + i (v_x h_1 + u_x h_2)}{h_1 + i h_2}.
\end{equation}

Now it suffices to show that 

\begin{equation}
    \lim_{h_1, h_2 \to 0} \frac{ u(h_1, h_2) - u(0, 0) - u_x h_1 + v_x h_2}{\sqrt{h_1^2 + h_2^2}} = 0
\end{equation}

for the real part, and similarly for the imaginary part. Because the C-R equations are satisfied, the same staircase argument works.

% To verify this, it's easiest to directly compute the derivative, namely $f' = u_x + iv_x$ and then verify that $Q \to 0$ as $h_1. h_2 \to 0$, in the same spirit as we did at the step (\ref{Q equation}), where
% \begin{equation}
    % Q = \frac{u(h_1, h_2) + iv(h_1, h_2) - u(0, 0) - i v(0, 0) - u_x(0, 0) - iv_x(0,0) }{\sqrt{h_1^2 + h_2^2}}
% \end{equation}
% (once more, we WLOG consider only differentiability at 0).
% Now decompose $|u(h_1, h_2) - u(0,0)| \le |u(h_1, h_2) - u(h_1, 0)| + |u(h_1, 0) - u(0,0)|$ and $|iv(h_1, h_2) - iv(0,0)| \le |iv(h_1, h_2) - iv(h_1, 0)| + |iv(h_1, 0) - iv(0,0)|$. We 
\end{proof}
\end{theorem}

Now back to less computational things. Look again at (\ref{derivvers}). There's nothing special about $||.||_2$! For our purposes, a more useful norm is the \vocab{operator norm}.

\begin{definition}
[Operator norm]

The \vocab{operator norm} on the space of linear maps $L(\RR^m, \RR^n)$ is the value

\begin{equation}
    \sup_{x \neq 0} \frac{||L(x)||_2}{||x||_2}.
\end{equation}
\end{definition}

This has the property that it is \emph{sub-multiplicative}, i.e $||AB|| \le ||A||||B||$ (exercise to reader).

\begin{example}
[2019 P1L]

The matrix function $f(M) = M^{-1}$ is differentiable at $I$.

\begin{proof}

    The following proof relies on the fact that the set of $n \times n$ invertible matrices is an open subset in the set of $n \times n$ matrices.

    This is quite a nice result in its own right, and follows from the following lemma

    \begin{lemma}
        The determinant map $\det : M_{n \times n}(\mathbb{R}) \to \RR$ is cts.

        \begin{proof}
            The map is simply a polynomial in the $n^2$ variables, which (check if unsure) are cts.
            % If we consider all matrix entries fixed except one, then $\det$ is a linear polynomial in that variable, and hence differentiable with respect to that variable. 
        \end{proof}
    \end{lemma}

    Due to this, since $\RR \setminus \{ 0 \}$ is open, so the set of invertible matrices is open since it is $\det^{-1}(\RR \setminus \{ 0 \})$.

    Back to the long question: we first (e.g by checking the one-dimensional case) convince ourselves that the answer is $-I$. This leaves us to verify that

    \begin{equation}
        \frac{ || (I+H)^{-1} - I + H|| }{|| H ||} \to 0.
    \end{equation}

    Now we can check that 

    \begin{equation}
        (I+H)^{-1} - I + H = H^2 (I + H)^{-1}.
    \end{equation}

    So using the sub-multiplicative property, since 
    
    \begin{equation}
        \frac{ || (I+H)^{-1} - I + H|| }{|| H ||} \le ||H (I+H)^{-1}|| \le ||H|| ||(I+H)^{-1}||
    \end{equation}

    we only need show that that $(I+H)^{-1}$ has bounded norm for sufficiently small $H$\footnote{I can't reason this part as cleanly as I hope the rest of the proof is reasoned!}. But this is true since suppose $||v||_2 = ||w||_2 = 1$ and 

    \begin{equation}
        (I+H)^{-1} v = \lambda w,
    \end{equation}

    Then

    \begin{equation}
        \frac{1}{\lambda} v = Iw + Hw.
    \end{equation}

    Now the magnitude of the RHS is going to arbitrarily close to 1 due to the bounded operator norm of $H$. So $\lambda$ can't grow large.
\end{proof}
\end{example}

% \begin{example}
% [2016 P2L]
% Letting $M_n$ be the set of $n \times n$ real matrices (in this setting, identified both with $L(\RR^n, \RR^n)$ and $\RR^{n^2}$) and $ V = \{ X \in M_n | ||X|| < 1 \} $ (where the norm $||.||$ is the operator norm), we have that the function $f : V \to M_n$ defined by % $U = \{ X \in M_n | (I-X) \text{invertible} \}$, we have that the function $f : U \to M_n$ defined by 
% \begin{equation}
    % X \mapsto (I-X)^{-1}
% \end{equation}
% is equal to $\sum_{k \ge 0} X^k$ and moreover is twice differentiable at 0.
% \begin{proof}
% As an initial sanity check, let's verify that $ V \oset M_n $. Suppose that $|v| = R < 1$. Then we have that 
% \end{proof}
% \end{example}

\section{Notation and Glossary}

\subsection{Notation}

\subsection{Glossary}

\begin{itemize}
    \item Cts: continuous.
\end{itemize}

\begin{thebibliography}{9}
    \bibitem{Napkin}
    Evan Chen (2021), \emph{An Infinitely Large Napkin}, \url{https://venhance.github.io/napkin/Napkin.pdf}.
\end{thebibliography}    

\end{document}